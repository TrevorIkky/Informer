{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Informer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TrevorIkky/Informer/blob/main/Informer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi4bh9Bb9jn5"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.config.run_functions_eagerly(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRD2dAfT95v6"
      },
      "source": [
        "# **Masks**\n",
        "Apply padding mask to values in encoder\n",
        "Apply combined mask (lookahead, padding) and padding mask to decoder. The combined mask applied to the first MHA, the padding mask applied to the second MHA in the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtfzr7HN8NUn"
      },
      "source": [
        "def padding_mask(sequence):\n",
        "  mask = tf.cast(tf.equal(sequence, 0), dtype=tf.float32)\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def look_ahead_mask(seq_len):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "  return mask\n",
        "\n",
        "\n",
        "def prob_lookahead_mask(B, H, L_Q, I, S):\n",
        "  mask = tf.ones([L_Q, S.shape[-1]], tf.float32)\n",
        "\n",
        "  mask = 1 - tf.linalg.band_part(mask, -1, 0)\n",
        "  mask_expanded = tf.broadcast_to(mask, [B, H, L_Q, S.shape[-1]])\n",
        "  #mask specific q based on reduced Q\n",
        "  mask_Q = tf.gather_nd(mask_expanded, I)\n",
        "  mask_Q = tf.cast(tf.reshape(mask_Q, S.shape), tf.bool)\n",
        "  return mask_Q\n",
        "\n",
        "def combined_mask(padding_mask, look_ahead_mask):\n",
        "  return tf.maximum(padding_mask, look_ahead_mask) \n",
        "\n",
        "# #Padding mask\n",
        "# S = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
        "# X = padding_mask(S)\n",
        "# Y = look_ahead_mask(S.shape[-1])\n",
        "# M = tf.maximum(X, Y)\n",
        "# print(f'Padding mask: {X}\\n\\nLook ahead mask {Y}\\n\\ntf.max: {M}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSEAc5Lq1pSp"
      },
      "source": [
        "# **Embedding Layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLtlmIKI8hJ9"
      },
      "source": [
        "from tensorflow.keras.initializers import HeNormal, Constant\n",
        "from tensorflow.keras.layers import Add, Conv1D, LeakyReLU, Embedding, Dropout, Layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzioa9dYlVf9"
      },
      "source": [
        "\"\"\"\n",
        "PositionalEncoding:\n",
        "Args:\n",
        "\n",
        "Notes:\n",
        "gY = 256, 50, gX = 256, 50\n",
        "np.sin.shape=(50,256) , np.cos.shape = (50,256)\n",
        "We are selecting every 2 pos. Therefore the total number of pos\n",
        "is 256 where at each pos we put the 50.\n",
        "\"\"\"\n",
        "\n",
        "class PositionalEmbedding(Layer):\n",
        "  def __init__(self, embedding_dim, max_seq_len=5000, **kwargs):\n",
        "    super(PositionalEmbedding, self).__init__()\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    if self.embedding_dim % 2 == 1 : self.embedding_dim += 1\n",
        "    gX, gY = np.meshgrid(np.arange(self.max_seq_len), np.arange(self.embedding_dim // 2)) #50, 256\n",
        "    pE = np.empty((1, self.max_seq_len, self.embedding_dim)) # 50, 512\n",
        "    print(pE[0, :, ::2].shape)\n",
        "    print(np.sin(gX / 10000**(2 * gY / self.embedding_dim)).shape)\n",
        "    pE[0, :, ::2] = np.sin(gX / 10000**(2 * gY / self.embedding_dim)).T\n",
        "    pE[0, :, 1::2] = np.cos(gX / 10000**(2 * gY / self.embedding_dim)).T\n",
        "    self.pE = tf.constant(pE, dtype=tf.float32)\n",
        "\n",
        "  def call(self, x):\n",
        "    x_shape = tf.shape(x)\n",
        "    return tf.tile(self.pE[:, :x_shape[-2], :], [x_shape[0], 1, 1])\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(PositionalEmbedding, self).get_config()\n",
        "    config.update({ \"max_seq_len\": self.max_seq_len, \"embedding_dim\": self.embedding_dim })\n",
        "    return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RieOc2DAia08",
        "outputId": "559fbb9c-16ab-40f7-e451-fb01a8d0bf27"
      },
      "source": [
        "pos_x = tf.random.uniform((32, 96, 7), minval=0, maxval=1)\n",
        "pos_emb = PositionalEmbedding(512)(pos_x)\n",
        "pos_emb.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 256)\n",
            "(256, 5000)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([32, 96, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ38ZNDflSuG"
      },
      "source": [
        "\"\"\"\n",
        "Args:\n",
        "tX = input\n",
        "dim_model = model dimension \n",
        "\n",
        "Notes:\n",
        "LeakyRelU = alpha * x if x < 0 else x if x > 0\n",
        "\"\"\"\n",
        "class TokenEmbedding(Layer):\n",
        "  def __init__(self, dim_model, **kwargs):\n",
        "    super(TokenEmbedding, self).__init__()\n",
        "    self.dim_model = dim_model\n",
        "    self.token_conv = Conv1D(dim_model, 3, padding='causal', activation= LeakyReLU(), kernel_initializer=HeNormal())\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = self.token_conv(x)\n",
        "    return x\n",
        "  \n",
        "  def get_config(self):\n",
        "    config = super(TokenEmbedding, self).get_config()\n",
        "    config.update({ \"dim_model\": self.dim_model })\n",
        "    return config\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQKHV3i8nUkq"
      },
      "source": [
        "\"\"\"\n",
        "Args:\n",
        "fX = input\n",
        "eI = encoder input = 7\n",
        "dim_model = model dimension = 512\n",
        "\"\"\"\n",
        "class FixedEmbedding(Layer):\n",
        "  def __init__(self, input_size, dim_model, **kwargs):\n",
        "    super(FixedEmbedding, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.dim_model = dim_model\n",
        "\n",
        "    if self.dim_model % 2 == 1 : self.dim_model += 1\n",
        "    gX, gY = np.meshgrid(np.arange(self.input_size), np.arange(self.dim_model // 2)) #50, 256\n",
        "    W = np.empty((1, self.input_size, self.dim_model)) # 50, 512\n",
        "    W[0, :, ::2] = np.sin(gX / 10000**(2 * gY / self.dim_model)).T\n",
        "    W[0, :, 1::2] = np.cos(gX / 10000**(2 * gY / self.dim_model)).T\n",
        "    W = tf.constant(W, dtype=tf.float32)\n",
        "    tf.stop_gradient(W)\n",
        "    W = Constant(W)\n",
        "\n",
        "    self.E = Embedding(self.input_size, self.dim_model, embeddings_initializer=W, trainable=False)\n",
        "\n",
        "  def call(self, x):\n",
        "    return self.E(x)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(FixedEmbedding, self).get_config()\n",
        "    config.update({ \"input_size\": self.input_size, \"dim_model\": self.dim_model })\n",
        "    return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJxrFLxepWn5"
      },
      "source": [
        "\"\"\"\n",
        "Args:\n",
        "pX = input\n",
        "dim_model = model_dimension = 512\n",
        "\"\"\"\n",
        "\n",
        "class TemporalEmbedding(Layer):\n",
        "  def __init__(self, dim_model=512, embed_type=\"fixed\", frequency=\"h\", **kwargs):\n",
        "    super(TemporalEmbedding, self).__init__()\n",
        "\n",
        "    self.embed_type = embed_type\n",
        "    self.frequency = frequency\n",
        "    \n",
        "    min = 4\n",
        "    hr = 24\n",
        "    wk = 7\n",
        "    dy = 32\n",
        "    mon = 13\n",
        "\n",
        "    Embed = FixedEmbedding if self.embed_type == 'fixed' else Embedding\n",
        "    \n",
        "    if self.frequency == \"m\":\n",
        "      self.minute_embed = Embed(min, dim_model)\n",
        "    self.hour_embed = Embed(hr, dim_model)\n",
        "    self.weekday_embed = Embed(wk, dim_model)\n",
        "    self.day_embed = Embed(dy, dim_model)\n",
        "    self.month_embed = Embed(mon, dim_model)\n",
        "\n",
        "  def call(self, x):\n",
        "    min_x = self.minute_embed(x[:, :, 4]) if hasattr(self, 'minute_embed') else 0.\n",
        "    hour_x = self.hour_embed(x[:, :, 3])\n",
        "    weekday_x = self.weekday_embed(x[:, :, 2])\n",
        "    day_x = self.day_embed(x[:, :, 1])\n",
        "    month_x = self.month_embed(x[:, :, 0])\n",
        "\n",
        "    return hour_x + weekday_x + day_x + month_x + min_x\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(TemporalEmbedding, self).get_config()\n",
        "    config.update({ \"embed_type\": self.embed_type, \"frequency\": self.frequency })\n",
        "    return config\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxO5Vr4TJRny"
      },
      "source": [
        "class TimeFeatureEmbedding(Layer):\n",
        "  def __init__(self, dim_model):\n",
        "    super(TimeFeatureEmbedding, self).__init__()\n",
        "    self.embedding = Dense(dim_model)\n",
        "  def call(self, x):\n",
        "    return self.embedding(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IrddgdUiViC"
      },
      "source": [
        "class DataEmbedding(Layer):\n",
        "  def __init__(self, dim_model=512, embed_type=\"time_features\", freq=\"h\", dropout_rate=0.1, training=False, **kwargs):\n",
        "    super(DataEmbedding, self).__init__()\n",
        "\n",
        "    self.dim_model = dim_model\n",
        "    self.embed_type = embed_type\n",
        "    self.freq = freq\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "    self.training = training\n",
        "\n",
        "    self.val_embedding = TokenEmbedding(self.dim_model)\n",
        "    \n",
        "    if embed_type != \"time_features\":\n",
        "      self.temp_embedding = TemporalEmbedding(self.dim_model, self.embed_type, self.freq)\n",
        "    else:\n",
        "      self.temp_embedding = TimeFeatureEmbedding(self.dim_model)\n",
        "    \n",
        "    self.pos_embedding = PositionalEmbedding(self.dim_model)\n",
        "\n",
        "    self.add = Add()\n",
        "    self.dropout = Dropout(self.dropout_rate)\n",
        "    \n",
        "  def call(self, x, x_mark):\n",
        "    embedding = self.add([self.val_embedding(x), self.temp_embedding(x_mark), self.pos_embedding(x)])\n",
        "    embedding = self.dropout(embedding, training=self.training)\n",
        "    return embedding\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(DataEmbedding, self).get_config()\n",
        "    config.update({ \"training\": self.training, \"dim_model\": self.dim_model, \"freq\": self.freq, \"dropout_rate\": self.dropout_rate })\n",
        "    return config\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l74ZoWu2zeRA"
      },
      "source": [
        "# data_embedding_x = tf.random.uniform((32, 96, 7), minval=0, maxval=1)\n",
        "# mark_embedding_x = tf.random.uniform((32, 96, 4), minval=0, maxval=7)\n",
        "# data_embedding_x = DataEmbedding(512, training=True)(data_embedding_x, mark_embedding_x)\n",
        "# data_embedding_x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TZ9bTCcwIjI"
      },
      "source": [
        "# **Multi-Head Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nesh5_6zDuH"
      },
      "source": [
        "from math import sqrt\n",
        "from typing import Tuple, List\n",
        "from tensorflow.keras.layers import Dense, Lambda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZmucbpFotKx"
      },
      "source": [
        "# Q = tf.random.uniform((4, 2, 2, 3), minval=0, maxval=1, dtype=tf.int32)\n",
        "# batch_indexes = tf.tile(tf.range(Q.shape[0])[:, tf.newaxis, tf.newaxis], (1, Q.shape[1], 2))\n",
        "# head_indexes = tf.tile(tf.range(Q.shape[1])[tf.newaxis, :, tf.newaxis], (Q.shape[0], 1, 2))\n",
        "# pre = tf.constant([[1, 1], [0, 1]])\n",
        "# pre = tf.tile(pre[tf.newaxis, : ], [Q.shape[0], 1, 1]) #2 = Batch size.\n",
        "# idx = tf.stack(values=[batch_indexes, head_indexes, pre], axis=-1)\n",
        "# # idx, batch_indexes, head_indexes\n",
        "# pre, idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeZXPMVPBkTS"
      },
      "source": [
        "**Vanilla Attention**<br>\n",
        "Args: K, V, Q, h, mask<br>\n",
        "K = Keys, V = values, Q = queries, mask=combined, lookahead, padding mask, none\n",
        "\n",
        "Output: Tuple(Tensor, Tensor)<br>\n",
        "Tensor1 = attention weights * values, shape = (B, L, D)<br>\n",
        "Tensor2 = attention weights, shape = (B, H, L_Q, L, V)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgstR-8owH5l"
      },
      "source": [
        "@tf.function\n",
        "def VanillaAttention(K, V, Q, h, dropout_rate=0.1, mask_flag=True, mask=None, training=True) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  sQ = tf.shape(Q)\n",
        "  # vB, vL_Q, vH, vE = Q.shape\n",
        "  B = sQ[0]\n",
        "  L_Q = sQ[1]\n",
        "  H = sQ[2]\n",
        "  E = tf.cast(sQ[-1], tf.float32)\n",
        "  \n",
        "  a = tf.einsum('nqhd, nkhd -> nhqk', Q, K)\n",
        "\n",
        "  if mask_flag:\n",
        "    if mask is not None:\n",
        "      a += (mask * -1e9)\n",
        "\n",
        "  x = a / tf.math.sqrt(E)\n",
        "  x = tf.nn.softmax(x, axis=-1)\n",
        "  x = Dropout(dropout_rate)(x, training=True)\n",
        "  x = tf.einsum('nhqk, nkhd -> nqhd', a, V)\n",
        " \n",
        "  x = tf.reshape(x, [B, L_Q, -1])\n",
        "  \n",
        "  return x , a    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu3244_wHdWC"
      },
      "source": [
        "**Prob Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYz1z0jW3eDw"
      },
      "source": [
        "@tf.function\n",
        "def ProbAttention(K, V, Q, c=5, mask_flag=True, output_attn=True) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "\n",
        "  B, L_K, H, E = K.shape\n",
        "  _, L_Q, _, _ = Q.shape\n",
        "  \n",
        "  p = [0, 2, 1, 3]\n",
        "\n",
        "  K = tf.transpose(K, perm=p)\n",
        "  Q = tf.transpose(Q, perm=p)\n",
        "  V = tf.transpose(V, perm=p)\n",
        "\n",
        "  #Sample keys to calculate query sparsity measurement on\n",
        "  #c = factor\n",
        "  U = c * np.ceil(np.log(L_K)).astype('int').item()\n",
        "  u = c * np.ceil(np.log(L_Q)).astype('int').item()\n",
        "\n",
        "  U = U if U < L_K else L_K\n",
        "  u = u if u < L_Q else L_Q\n",
        "\n",
        "  #Top scores, Query Indices\n",
        "  S, Q_I = prob_QK(K, Q, U, u)\n",
        "  S = S * 1./sqrt(E)\n",
        " \n",
        "  context = get_initial_context(mask_flag, V, L_Q)\n",
        "\n",
        "  context, A = update_context(context, S, V, Q_I, L_Q, mask_flag, output_attn)\n",
        "\n",
        "  return context, A\n",
        "\n",
        "def prob_QK(K, Q, sample_k, u) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  B, H, L_K, E = K.shape\n",
        "  _, _, L_Q, _ = Q.shape\n",
        "\n",
        "  K_E = tf.broadcast_to(tf.expand_dims(K, axis=-3), [B, H, L_K, L_Q, E])\n",
        "  #sample_k = c * ln(L_K)\n",
        "  K_rand_idx = tf.random.uniform((L_Q, sample_k), maxval=L_K, dtype=tf.int32)\n",
        "\n",
        "  K_rand_idx = tf.tile(K_rand_idx[tf.newaxis, tf.newaxis, : ], [B, H, 1, 1]) #2 = Batch size.\n",
        "\n",
        "  batch_indexes = tf.tile(tf.range(B)[:, tf.newaxis, tf.newaxis, tf.newaxis], (1, H, L_K, K_rand_idx.shape[-1]))\n",
        "  head_indexes = tf.tile(tf.range(H)[tf.newaxis, :, tf.newaxis, tf.newaxis], (B, 1, L_K, K_rand_idx.shape[-1]))\n",
        "  k_indexes = tf.tile(tf.range(L_K)[tf.newaxis, tf.newaxis, :, tf.newaxis ], (B, H, 1, K_rand_idx.shape[-1]))\n",
        " \n",
        "  \n",
        "  K_rand_idx = tf.stack(values=[batch_indexes, head_indexes, k_indexes, K_rand_idx], axis=-1)\n",
        "  \n",
        "  K_sample = tf.gather_nd(K_E, K_rand_idx)\n",
        " \n",
        "  Q_K_sample = tf.squeeze(tf.matmul(tf.expand_dims(Q, axis=-2), tf.transpose(K_sample, [0, 1, 2, 4, 3])))\n",
        "\n",
        "  #u = c * ln(L_Q)\n",
        "  M = tf.reduce_max(Q_K_sample, axis=-1) - tf.divide(tf.reduce_sum(Q_K_sample, axis=-1), L_K)\n",
        "  \n",
        "  M_top = tf.math.top_k(M, u, sorted=False)[1]\n",
        "\n",
        "  M_top = M_top[tf.newaxis] if B == 1 else M_top\n",
        "\n",
        "  M_top = tf.tile(M_top, (1, 1, 1)) \n",
        "  \n",
        "  batch_indexes = tf.tile(tf.range(B)[:, tf.newaxis, tf.newaxis], (1, H, u))\n",
        "  head_indexes = tf.tile(tf.range(H)[tf.newaxis, :, tf.newaxis], (B, 1, u))\n",
        "  \n",
        "  rQ_idx = tf.stack(values=[batch_indexes, head_indexes, M_top], axis=-1)\n",
        "\n",
        "  #Q_K interactions = L_K * ln (L_Q) * c (drop the constant)\n",
        "  #                 = L_K * ln (L_Q)\n",
        "\n",
        "  R_Q = tf.gather_nd(Q, rQ_idx)\n",
        "  Q_K = tf.matmul(R_Q, tf.transpose(K, perm=[0, 1, 3, 2]))\n",
        "\n",
        "  return Q_K, M_top\n",
        "\n",
        "\n",
        "def get_initial_context(mask_flag, V, L_Q) -> tf.Tensor:\n",
        "  B, H, L_V, E = V.shape \n",
        "\n",
        "  if not mask_flag: #context returned is same shape as Q\n",
        "    m = tf.math.reduce_mean(V, axis=-2)\n",
        "    context = tf.identity(tf.broadcast_to(tf.expand_dims(m, axis=-2), [B, H, L_Q, m.shape[-1]]))\n",
        "  else: #Context returned is same shape as V\n",
        "    assert L_Q == L_V, \"Initial context: L_Q != L_V\"\n",
        "    context = tf.math.cumsum(V, axis=-2)\n",
        "  \n",
        "  return context\n",
        "\n",
        "def update_context(context_in, scores, V, Q_I, L_Q, mask_flag=True, output_attn=True) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  B, H, L_V, E = V.shape\n",
        "\n",
        "  batch_indexes = tf.tile(tf.range(B)[:, tf.newaxis, tf.newaxis], (1, H, Q_I.shape[-1]))\n",
        "  head_indexes = tf.tile(tf.range(H)[tf.newaxis, :, tf.newaxis], (B, 1, Q_I.shape[-1]))\n",
        "\n",
        "  ctx_I = tf.stack(values=[batch_indexes, head_indexes, Q_I], axis=-1)\n",
        "\n",
        "  if mask_flag:\n",
        "    mask = prob_lookahead_mask(B, H, L_Q, ctx_I, scores)\n",
        "    scores = tf.where(mask, -np.inf, scores)\n",
        "\n",
        "  a = tf.nn.softmax(scores, axis=-1)\n",
        "  x = tf.matmul(a, V) #shape: B, H, Q, D\n",
        " \n",
        "  context_in = tf.tensor_scatter_nd_update(context_in, ctx_I, x)\n",
        "\n",
        "  context_in = tf.transpose(context_in, [0, 2, 1, 3])\n",
        "\n",
        "  context_in = tf.reshape(context_in, [B, L_Q, -1])\n",
        " \n",
        "  if output_attn:\n",
        "    attns = (tf.ones((B, H, L_V, L_V)) / L_V)\n",
        "    attns = tf.tensor_scatter_nd_update(attns, ctx_I, a)\n",
        "    return context_in, attns\n",
        "  else:\n",
        "    return context_in, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwIf9uxcbdIG"
      },
      "source": [
        "# pX = tf.constant([[[[-0.5034, -1.1429,  0.4153],\n",
        "#           [ 0.3051,  0.1895, -2.0370]],\n",
        "\n",
        "#          [[-0.2590, -0.6820, -0.2117],\n",
        "#           [-0.8491, -1.2988, -0.3452]]]])\n",
        "# pX = ProbAttention(pX, pX, pX, mask_flag=True)[0]\n",
        "# pX.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ64vOTdwpw2"
      },
      "source": [
        "@tf.function\n",
        "def AttentionLayer(K, V, Q, att=\"v\", dim_model=512, heads=8, factor=5, mask=None, mask_flag=True) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "\n",
        "  sK = tf.shape(K)\n",
        "  sQ = tf.shape(Q)\n",
        "  sV = tf.shape(V)\n",
        "\n",
        "  E = sK[-1]\n",
        "  B = sK[0]\n",
        "  aB = sQ[0]\n",
        "\n",
        "  L_K = sK[1]\n",
        "  L_Q = sQ[1]\n",
        "  L_V = sV[1]\n",
        "\n",
        "\n",
        "  assert dim_model % heads == 0, \"Dim Model % Heads != 0\"\n",
        "\n",
        "  K = Dense(dim_model)(K)\n",
        "  V = Dense(dim_model)(V)\n",
        "  Q = Dense(dim_model)(Q)\n",
        "\n",
        "  D = dim_model // heads\n",
        "\n",
        "  K = tf.reshape(K, [B, L_K, heads, D])\n",
        "  V = tf.reshape(V, [B, L_V, heads, D])\n",
        "  Q = tf.reshape(Q, [aB, L_Q, heads, D])\n",
        "\n",
        "  if att == \"v\":\n",
        "    x, a = VanillaAttention(K, V, Q, heads, mask_flag=mask_flag, mask=mask)\n",
        "  else:\n",
        "    x, a = ProbAttention(K, V, Q, mask_flag=mask_flag, output_attn=True)\n",
        " \n",
        "  x = Dense(dim_model)(x)\n",
        "  \n",
        "  return x, a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reomkBhOPhVW",
        "outputId": "525d2424-5680-4157-8e3e-01cd6a20a922"
      },
      "source": [
        "x = tf.random.uniform((2, 2, 512), minval=0, maxval=1)\n",
        "o, a = AttentionLayer(x, x, x, att=\"p\", dim_model=512, mask_flag=False)\n",
        "o.shape, a.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([2, 2, 512]), TensorShape([2, 8, 2, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uoj-7ktQp24w"
      },
      "source": [
        "# z = tf.random.uniform((20, 4, 4, 4), minval=0, maxval=1)\n",
        "# pre = tf.constant([[0, 1, 0], [1, 1, 0], [1, 1, 0], [1, 1, 0]])\n",
        "\n",
        "# pre = tf.tile(pre[tf.newaxis, tf.newaxis, : ], [z.shape[0], z.shape[1], 1, 1]) #2 = Batch size.\n",
        "\n",
        "# batch_indexes = tf.tile(tf.range(z.shape[0])[:, tf.newaxis, tf.newaxis, tf.newaxis], (1, z.shape[1], z.shape[2], pre.shape[-1]))\n",
        "# head_indexes = tf.t\n",
        "#     M_top =ile(tf.range(z.shape[1])[tf.newaxis, :, tf.newaxis, tf.newaxis], (z.shape[0], 1, z.shape[2], pre.shape[-1]))\n",
        "# k_indexes = tf.tile(tf.range(z.shape[2])[tf.newaxis, tf.newaxis, :, tf.newaxis ], (z.shape[0], z.shape[1], 1, pre.shape[-1]))\n",
        "\n",
        "\n",
        "# idx = tf.stack(values=[batch_indexes, head_indexes, k_indexes, pre], axis=-1)\n",
        "\n",
        "# zS = tf.gather_nd(z, idx)\n",
        "# # z, idx, zS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_9nbnpP38c_"
      },
      "source": [
        "# **Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLLKWwM_Mf5t"
      },
      "source": [
        "from tensorflow.keras.layers import MaxPool1D, ELU, BatchNormalization, LayerNormalization, ReLU, Add"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swv3GpavIdw2"
      },
      "source": [
        "\"\"\"\n",
        "Args:\n",
        "x = Input Tensor\n",
        "C = In channels (# of filters)\n",
        "\n",
        "Todo: Check ELU & BatchNormalization\n",
        "\"\"\"\n",
        "@tf.function\n",
        "def DistilLayer(x, dim_model) -> tf.Tensor:\n",
        "  x = Conv1D(dim_model, 3, padding=\"causal\")(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ELU()(x)\n",
        "  x = MaxPool1D(pool_size=3, strides=2)(x)\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkCRV7Hq38JC"
      },
      "source": [
        "@tf.function\n",
        "def EncoderLayer(x, dim_model=512, expansion=4, activation=\"gelu\", attn=\"p\", dropout_rate=0.1, training=False, attn_mask=None) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  nx, eA = AttentionLayer(x, x, x, attn, dim_model, mask_flag=False, mask=attn_mask)\n",
        "  nx = Dropout(dropout_rate)(nx, training=training)\n",
        "  x = Add()([x, nx])\n",
        "  y = x = LayerNormalization()(x)\n",
        "\n",
        "  y = Conv1D(dim_model * expansion, kernel_size=1)(y)\n",
        "  \n",
        "  y = ReLU()(y) if activation==\"relu\" else ELU()(y)\n",
        "  y = Dropout(dropout_rate)(y, training=training)\n",
        "  \n",
        "  y = Conv1D(dim_model, kernel_size=1)(y)\n",
        "  y = Dropout(dropout_rate)(y, training=training)\n",
        "  y = LayerNormalization()(x + y)\n",
        "  return y, eA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtIeKBinMnFW",
        "outputId": "f3ee66d4-9919-4795-89db-abe1ce010077"
      },
      "source": [
        "\"\"\"\n",
        "Encoder Layer Test\n",
        "\"\"\"\n",
        "x = tf.random.uniform((32, 64, 512), minval=0, maxval=1)\n",
        "encodings_y, encodings_a = EncoderLayer(x, 512, training=True)\n",
        "encodings_y.shape, encodings_a.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([32, 64, 512]), TensorShape([32, 8, 64, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2Qpp_0UMZI1"
      },
      "source": [
        "@tf.function\n",
        "def Encoder(x, enc_layers: List[int], conv_layers: List[int], attn=\"p\", dim_model=512, expansion=4, dropout_rate=0.1, activation=\"gelu\", training=False, attn_mask=None, normalize=True) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  attns = []\n",
        "  if conv_layers is not None:\n",
        "    for i, j in zip(enc_layers, conv_layers):\n",
        "      x, t = EncoderLayer(x, dim_model, expansion, activation, attn, dropout_rate, training, attn_mask)\n",
        "      x = DistilLayer(x, dim_model)\n",
        "      attns.append(t)\n",
        "    x, t = EncoderLayer(x, dim_model, expansion, activation, attn, dropout_rate, training, attn_mask)\n",
        "    attns.append(t)\n",
        "  else:\n",
        "    for i in enc_layers:\n",
        "      x, t = EncoderLayer(x, dim_model, expansion, activation, attn, dropout_rate, training, attn_mask)\n",
        "      attns.append(t)\n",
        "  if normalize:\n",
        "    x = LayerNormalization()(x)\n",
        "  return x, attns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPzVB2ZkozzS"
      },
      "source": [
        "\"\"\"\n",
        "Encoder Test\n",
        "\"\"\"\n",
        "enc_layers = [i for i in range(2)]\n",
        "conv_layers = [i for i in range(1)]\n",
        "encoder_x, encoder_l = Encoder(x, enc_layers, conv_layers, training=True)\n",
        "encoder_x.shape, len(encoder_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-15hu6xeU0O9"
      },
      "source": [
        "# **Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugIuORiYU24c"
      },
      "source": [
        "\"\"\"\n",
        "Args:\n",
        "x_mask for attn 1 (Prob Attention)\n",
        "cross_mask for attn 2 (Vanilla Attention)\n",
        "E = encoder output\n",
        "\"\"\"\n",
        "@tf.function\n",
        "def DecoderLayer(x, E, dim_model=512, heads=8, factor=5, expansion=4, attn=\"p\", activation=\"relu\", dropout_rate=0.1, x_mask=None, cross_mask=None, training=False,) -> tf.Tensor:\n",
        "  z = AttentionLayer(x, x, x, attn, dim_model, heads, factor, x_mask, True)[0]\n",
        "  z = Dropout(dropout_rate)(z, training=training)\n",
        "  x = Add()([x, z])\n",
        "  x = LayerNormalization()(x)\n",
        "\n",
        "  attn = \"v\"\n",
        "  y = AttentionLayer(E, E, x, attn, dim_model, heads, factor, cross_mask, False)[0]\n",
        "  y = Dropout(dropout_rate)(y, training=training)\n",
        "  x = Add()([x, y])\n",
        "  y = x = LayerNormalization()(x)\n",
        "\n",
        "  y = Conv1D(dim_model * expansion, kernel_size=1)(y)\n",
        "  y = ReLU()(y) if activation==\"relu\" else ELU()(y)\n",
        "  y = Dropout(dropout_rate)(y, training=training)\n",
        "  \n",
        "  y = Conv1D(dim_model, kernel_size=1)(y)\n",
        "  y = Dropout(dropout_rate)(y, training=training)\n",
        "  y = LayerNormalization()(x + y)\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1q9GEaQpzf4",
        "outputId": "c0b0b93c-9433-43e0-f216-1c1a464b9c85"
      },
      "source": [
        "\"\"\"\n",
        "Decoder Layer Test\n",
        "\"\"\"\n",
        "x = tf.random.uniform((32, 64, 512), minval=0, maxval=1)\n",
        "decoding_y = DecoderLayer(x, encoder_x, training=True)\n",
        "decoding_y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([32, 64, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh24D54JVGAv"
      },
      "source": [
        "@tf.function\n",
        "def Decoder(x, E, dec_layers:List[int], dim_model=512, heads=8, attn=\"p\", factor=5, expansion=4, dropout_rate=0.1, activation=\"gelu\", training=False, x_mask=None, cross_mask=None, normalize=True) -> tf.Tensor:\n",
        "  for k in dec_layers:\n",
        "    x = DecoderLayer(x, E, dim_model, heads, factor, expansion, attn, activation, dropout_rate, x_mask, cross_mask, training)\n",
        "  if normalize:\n",
        "    x = LayerNormalization()(x)\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZMEaYaO7baL",
        "outputId": "89bd943c-297a-4f3d-aeb0-0ee3ccdf2bcf"
      },
      "source": [
        "\"\"\"\n",
        "Decoder Test\n",
        "\"\"\"\n",
        "decoder_layers = enc_layers = [i for i in range(3)]\n",
        "x = tf.random.uniform((32, 64, 512), minval=0, maxval=1)\n",
        "decoder_y = Decoder(x, encoder_x, decoder_layers, training=True)\n",
        "decoder_y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([32, 64, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBRgAA7abBig"
      },
      "source": [
        "# **Informer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5EYBa9KbN89"
      },
      "source": [
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras import Input, Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p31uNW1mbIEX"
      },
      "source": [
        "\"\"\"\n",
        "Args\n",
        "x = input \n",
        "y = target\n",
        "\n",
        "Notes:\n",
        "The mask for ProbAttention is already defined within the attention\n",
        "The mask passed to the Encoder is an input padding mask to mask out zero values\n",
        "in the input sequence\n",
        "\"\"\"\n",
        "@tf.function\n",
        "def Informer(dim_model=512, heads=8, enc_layers:int=2,  dec_layers:int=1, attn=\"p\", expansion=4, factor = 5, dropout_rate=0.05, activation=\"gelu\", embed_type=\"fixed\", frequency=\"h\", training=False, x_mask=None, cross_mask=None) -> Model:\n",
        "  encoder_x = Input(batch_input_shape=(32, 96, 7))\n",
        "  encoder_x_mark = Input(batch_input_shape=(32, 96, 4))\n",
        "\n",
        "  decoder_x = Input(batch_input_shape=(32, 72, 7))\n",
        "  decoder_x_mark = Input(batch_input_shape=(32, 72, 4))\n",
        "\n",
        "\n",
        "  encoder_embedding_x = DataEmbedding(dim_model, embed_type, frequency, dropout_rate, training)(encoder_x, encoder_x_mark)\n",
        "\n",
        "  conv_layers = [ conv_idx for conv_idx in range(enc_layers - 1) ]\n",
        "  enc_layers = [ enc_idx for enc_idx in range(enc_layers) ]\n",
        "\n",
        "  enc_attn_mask = Lambda(padding_mask, name=\"enc_attn_mask\")(encoder_x)\n",
        "\n",
        "  E, mh_attentions = Encoder(encoder_embedding_x, enc_layers , conv_layers, attn, dim_model, expansion, \n",
        "                  dropout_rate, activation, training, enc_attn_mask, True)\n",
        "  \n",
        "  decoder_embedding_x = DataEmbedding(dim_model, embed_type, frequency, dropout_rate, training)(decoder_x, decoder_x_mark)\n",
        "\n",
        "  dec_layers = [ dec_idx for dec_idx in range(dec_layers)]\n",
        "\n",
        "  decoder_output = Decoder(decoder_embedding_x, E, dec_layers, dim_model, heads, attn, factor, expansion,\n",
        "              dropout_rate, activation, training, x_mask, cross_mask, True)\n",
        "  \n",
        "  output = Dense(7)(decoder_output)\n",
        "  output = Lambda(lambda x: x[:, -24: ,:])(output)\n",
        "  model = Model(inputs=[encoder_x, decoder_x, encoder_x_mark, decoder_x_mark], outputs=output, name=\"Informer\")\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ0B-irL80AW",
        "outputId": "77cb6985-d9a7-4e01-bf48-b224b3b250ac"
      },
      "source": [
        "model = Informer(enc_layers=1, dec_layers=1, training=True)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Informer\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(32, 96, 7)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(32, 96, 4)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "data_embedding (DataEmbedding)  (32, 96, 512)        50176       input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.shape (TFOpLambda) (3,)                 0           data_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_1 (Sli ()                   0           tf.compat.v1.shape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dense_48 (Dense)                (32, 96, 512)        262656      data_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3 (Sli ()                   0           tf.compat.v1.shape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.reshape (TFOpLambda)         (32, 96, 8, 64)      0           dense_48[0][0]                   \n",
            "                                                                 tf.__operators__.getitem_1[0][0] \n",
            "                                                                 tf.__operators__.getitem_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.shape_1 (TFOpLambd (3,)                 0           data_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose (TFOpLam (32, 8, 96, 64)      0           tf.reshape[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_50 (Dense)                (32, 96, 512)        262656      data_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_2 (Sli ()                   0           tf.compat.v1.shape_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_4 (Sli ()                   0           tf.compat.v1.shape_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims (TFOpLambda)     (32, 8, 1, 96, 64)   0           tf.compat.v1.transpose[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(32, 72, 7)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(32, 72, 4)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.reshape_2 (TFOpLambda)       (32, 96, 8, 64)      0           dense_50[0][0]                   \n",
            "                                                                 tf.__operators__.getitem_2[0][0] \n",
            "                                                                 tf.__operators__.getitem_4[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf.broadcast_to (TFOpLambda)    (32, 8, 96, 96, 64)  0           tf.expand_dims[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "data_embedding_1 (DataEmbedding (32, 72, 512)        50176       input_3[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_1 (TFOpL (32, 8, 96, 64)      0           tf.reshape_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.gather_nd (TFOpLam (32, 8, 96, 25, 64)  0           tf.broadcast_to[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.shape_3 (TFOpLambd (3,)                 0           data_embedding_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_1 (TFOpLambda)   (32, 8, 96, 1, 64)   0           tf.compat.v1.transpose_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_3 (TFOpL (32, 8, 96, 64, 25)  0           tf.compat.v1.gather_nd[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_7 (Sli ()                   0           tf.compat.v1.shape_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_52 (Dense)                (32, 72, 512)        262656      data_embedding_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_9 (Sli ()                   0           tf.compat.v1.shape_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.linalg.matmul (TFOpLambda)   (32, 8, 96, 1, 25)   0           tf.expand_dims_1[0][0]           \n",
            "                                                                 tf.compat.v1.transpose_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.reshape_4 (TFOpLambda)       (32, 72, 8, 64)      0           dense_52[0][0]                   \n",
            "                                                                 tf.__operators__.getitem_7[0][0] \n",
            "                                                                 tf.__operators__.getitem_9[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.squeeze (TFOpLambd (32, 8, 96, 25)      0           tf.linalg.matmul[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.shape_4 (TFOpLambd (3,)                 0           data_embedding_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_6 (TFOpL (32, 8, 72, 64)      0           tf.reshape_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.reduce_sum (TFOpLambda) (32, 8, 96)          0           tf.compat.v1.squeeze[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_54 (Dense)                (32, 72, 512)        262656      data_embedding_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_8 (Sli ()                   0           tf.compat.v1.shape_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_10 (Sl ()                   0           tf.compat.v1.shape_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_3 (TFOpLambda)   (32, 8, 1, 72, 64)   0           tf.compat.v1.transpose_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.reduce_max (TFOpLambda) (32, 8, 96)          0           tf.compat.v1.squeeze[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.truediv (TFOpLambda)    (32, 8, 96)          0           tf.math.reduce_sum[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.reshape_6 (TFOpLambda)       (32, 72, 8, 64)      0           dense_54[0][0]                   \n",
            "                                                                 tf.__operators__.getitem_8[0][0] \n",
            "                                                                 tf.__operators__.getitem_10[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf.broadcast_to_2 (TFOpLambda)  (32, 8, 72, 72, 64)  0           tf.expand_dims_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.subtract (TFOpLambda)   (32, 8, 96)          0           tf.math.reduce_max[0][0]         \n",
            "                                                                 tf.math.truediv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_7 (TFOpL (32, 8, 72, 64)      0           tf.reshape_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.gather_nd_2 (TFOpL (32, 8, 72, 25, 64)  0           tf.broadcast_to_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.top_k (TFOpLambda)      TopKV2(values=(32, 8 0           tf.math.subtract[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_4 (TFOpLambda)   (32, 8, 72, 1, 64)   0           tf.compat.v1.transpose_7[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_9 (TFOpL (32, 8, 72, 64, 25)  0           tf.compat.v1.gather_nd_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.shape_2 (TFOpLambd (3,)                 0           data_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.tile (TFOpLambda)            (32, 8, 25)          0           tf.math.top_k[0][1]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.linalg.matmul_3 (TFOpLambda) (32, 8, 72, 1, 25)   0           tf.expand_dims_4[0][0]           \n",
            "                                                                 tf.compat.v1.transpose_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense_49 (Dense)                (32, 96, 512)        262656      data_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_5 (Sli ()                   0           tf.compat.v1.shape_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.stack (TFOpLambda)           (32, 8, 25, 3)       0           tf.tile[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.squeeze_1 (TFOpLam (32, 8, 72, 25)      0           tf.linalg.matmul_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.reshape_1 (TFOpLambda)       (32, 96, 8, 64)      0           dense_49[0][0]                   \n",
            "                                                                 tf.__operators__.getitem_1[0][0] \n",
            "                                                                 tf.__operators__.getitem_5[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.gather_nd_1 (TFOpL (32, 8, 25, 64)      0           tf.compat.v1.transpose_1[0][0]   \n",
            "                                                                 tf.stack[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_4 (TFOpL (32, 8, 64, 96)      0           tf.compat.v1.transpose[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.reduce_sum_1 (TFOpLambd (32, 8, 72)          0           tf.compat.v1.squeeze_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_2 (TFOpL (32, 8, 96, 64)      0           tf.reshape_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.linalg.matmul_1 (TFOpLambda) (32, 8, 25, 96)      0           tf.compat.v1.gather_nd_1[0][0]   \n",
            "                                                                 tf.compat.v1.transpose_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.reduce_max_1 (TFOpLambd (32, 8, 72)          0           tf.compat.v1.squeeze_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.truediv_2 (TFOpLambda)  (32, 8, 72)          0           tf.math.reduce_sum_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.reduce_mean (TFOpLambda (32, 8, 64)          0           tf.compat.v1.transpose_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.multiply (TFOpLambda)   (32, 8, 25, 96)      0           tf.linalg.matmul_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.subtract_1 (TFOpLambda) (32, 8, 72)          0           tf.math.reduce_max_1[0][0]       \n",
            "                                                                 tf.math.truediv_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_2 (TFOpLambda)   (32, 8, 1, 64)       0           tf.math.reduce_mean[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.truediv_1 (TFOpLambda)  (32, 8, 25, 96)      0           tf.math.multiply[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.top_k_1 (TFOpLambda)    TopKV2(values=(32, 8 0           tf.math.subtract_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.broadcast_to_1 (TFOpLambda)  (32, 8, 96, 64)      0           tf.expand_dims_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.softmax (TFOpLambda)      (32, 8, 25, 96)      0           tf.math.truediv_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf.tile_1 (TFOpLambda)          (32, 8, 25)          0           tf.math.top_k_1[0][1]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.identity (TFOpLambda)        (32, 8, 96, 64)      0           tf.broadcast_to_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf.stack_1 (TFOpLambda)         (32, 8, 25, 3)       0           tf.tile[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.linalg.matmul_2 (TFOpLambda) (32, 8, 25, 64)      0           tf.nn.softmax[0][0]              \n",
            "                                                                 tf.compat.v1.transpose_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.stack_2 (TFOpLambda)         (32, 8, 25, 3)       0           tf.tile_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.tensor_scatter_nd_update (TF (32, 8, 96, 64)      0           tf.identity[0][0]                \n",
            "                                                                 tf.stack_1[0][0]                 \n",
            "                                                                 tf.linalg.matmul_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.stack_3 (TFOpLambda)         (32, 8, 25, 3)       0           tf.tile_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.gather_nd_3 (TFOpL (32, 8, 25, 64)      0           tf.compat.v1.transpose_7[0][0]   \n",
            "                                                                 tf.stack_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_10 (TFOp (32, 8, 64, 72)      0           tf.compat.v1.transpose_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_5 (TFOpL (32, 96, 8, 64)      0           tf.tensor_scatter_nd_update[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.gather_nd_4 (TFOpL (32, 8, 25, 72)      0           tf.stack_3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf.linalg.matmul_4 (TFOpLambda) (32, 8, 25, 72)      0           tf.compat.v1.gather_nd_3[0][0]   \n",
            "                                                                 tf.compat.v1.transpose_10[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "tf.reshape_3 (TFOpLambda)       (32, 96, 512)        0           tf.compat.v1.transpose_5[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.shape_5 (TFOpLambd (3,)                 0           data_embedding_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.reshape_7 (TFOpLambda)       (32, 8, 25, 72)      0           tf.compat.v1.gather_nd_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.multiply_1 (TFOpLambda) (32, 8, 25, 72)      0           tf.linalg.matmul_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dense_51 (Dense)                (32, 96, 512)        262656      tf.reshape_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_53 (Dense)                (32, 72, 512)        262656      data_embedding_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_11 (Sl ()                   0           tf.compat.v1.shape_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.cast (TFOpLambda)            (32, 8, 25, 72)      0           tf.reshape_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.truediv_3 (TFOpLambda)  (32, 8, 25, 72)      0           tf.math.multiply_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (32, 96, 512)        0           dense_51[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf.reshape_5 (TFOpLambda)       (32, 72, 8, 64)      0           dense_53[0][0]                   \n",
            "                                                                 tf.__operators__.getitem_7[0][0] \n",
            "                                                                 tf.__operators__.getitem_11[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf.where (TFOpLambda)           (32, 8, 25, 72)      0           tf.cast[0][0]                    \n",
            "                                                                 tf.math.truediv_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (32, 96, 512)        0           data_embedding[0][0]             \n",
            "                                                                 dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_8 (TFOpL (32, 8, 72, 64)      0           tf.reshape_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.softmax_1 (TFOpLambda)    (32, 8, 25, 72)      0           tf.where[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_20 (LayerNo (32, 96, 512)        1024        add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.cumsum (TFOpLambda)     (32, 8, 72, 64)      0           tf.compat.v1.transpose_8[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.linalg.matmul_5 (TFOpLambda) (32, 8, 25, 64)      0           tf.nn.softmax_1[0][0]            \n",
            "                                                                 tf.compat.v1.transpose_8[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_16 (Conv1D)              (32, 96, 2048)       1050624     layer_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.tensor_scatter_nd_update_2 ( (32, 8, 72, 64)      0           tf.math.cumsum[0][0]             \n",
            "                                                                 tf.stack_3[0][0]                 \n",
            "                                                                 tf.linalg.matmul_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "elu_7 (ELU)                     (32, 96, 2048)       0           conv1d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_11 (TFOp (32, 72, 8, 64)      0           tf.tensor_scatter_nd_update_2[0][\n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (32, 96, 2048)       0           elu_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.reshape_8 (TFOpLambda)       (32, 72, 512)        0           tf.compat.v1.transpose_11[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_17 (Conv1D)              (32, 96, 512)        1049088     dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_55 (Dense)                (32, 72, 512)        262656      tf.reshape_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (32, 96, 512)        0           conv1d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (32, 72, 512)        0           dense_55[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add (TFOpLambd (32, 96, 512)        0           layer_normalization_20[0][0]     \n",
            "                                                                 dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (32, 72, 512)        0           data_embedding_1[0][0]           \n",
            "                                                                 dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_21 (LayerNo (32, 96, 512)        1024        tf.__operators__.add[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_23 (LayerNo (32, 72, 512)        1024        add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_22 (LayerNo (32, 96, 512)        1024        layer_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.shape_7 (TFOpLambd (3,)                 0           layer_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.shape_6 (TFOpLambd (3,)                 0           layer_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_58 (Dense)                (32, 72, 512)        262656      layer_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_14 (Sl ()                   0           tf.compat.v1.shape_7[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_16 (Sl ()                   0           tf.compat.v1.shape_7[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_56 (Dense)                (32, 96, 512)        262656      layer_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_13 (Sl ()                   0           tf.compat.v1.shape_6[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_15 (Sl ()                   0           tf.compat.v1.shape_6[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.shape_8 (TFOpLambd (3,)                 0           layer_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.reshape_11 (TFOpLambda)      (32, 72, 8, 64)      0           dense_58[0][0]                   \n",
            "                                                                 tf.__operators__.getitem_14[0][0]\n",
            "                                                                 tf.__operators__.getitem_16[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf.reshape_9 (TFOpLambda)       (32, 96, 8, 64)      0           dense_56[0][0]                   \n",
            "                                                                 tf.__operators__.getitem_13[0][0]\n",
            "                                                                 tf.__operators__.getitem_15[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_57 (Dense)                (32, 96, 512)        262656      layer_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_17 (Sl ()                   0           tf.compat.v1.shape_8[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.einsum (TFOpLambda)          (32, 8, 72, 96)      0           tf.reshape_11[0][0]              \n",
            "                                                                 tf.reshape_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.reshape_10 (TFOpLambda)      (32, 96, 8, 64)      0           dense_57[0][0]                   \n",
            "                                                                 tf.__operators__.getitem_13[0][0]\n",
            "                                                                 tf.__operators__.getitem_17[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.shape_9 (TFOpLambd (4,)                 0           tf.reshape_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.einsum_1 (TFOpLambda)        (32, 72, 8, 64)      0           tf.einsum[0][0]                  \n",
            "                                                                 tf.reshape_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_18 (Sl ()                   0           tf.compat.v1.shape_9[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_19 (Sl ()                   0           tf.compat.v1.shape_9[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.reshape_12 (TFOpLambda)      (32, 72, 512)        0           tf.einsum_1[0][0]                \n",
            "                                                                 tf.__operators__.getitem_18[0][0]\n",
            "                                                                 tf.__operators__.getitem_19[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_59 (Dense)                (32, 72, 512)        262656      tf.reshape_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (32, 72, 512)        0           dense_59[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (32, 72, 512)        0           layer_normalization_23[0][0]     \n",
            "                                                                 dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_24 (LayerNo (32, 72, 512)        1024        add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_19 (Conv1D)              (32, 72, 2048)       1050624     layer_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "elu_8 (ELU)                     (32, 72, 2048)       0           conv1d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (32, 72, 2048)       0           elu_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_20 (Conv1D)              (32, 72, 512)        1049088     dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (32, 72, 512)        0           conv1d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_1 (TFOpLam (32, 72, 512)        0           layer_normalization_24[0][0]     \n",
            "                                                                 dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_25 (LayerNo (32, 72, 512)        1024        tf.__operators__.add_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_26 (LayerNo (32, 72, 512)        1024        layer_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_60 (Dense)                (32, 72, 7)          3591        layer_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (32, 24, 7)          0           dense_60[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 7,462,407\n",
            "Trainable params: 7,384,583\n",
            "Non-trainable params: 77,824\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pVWCKVQEA1b"
      },
      "source": [
        "# **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXV-cLbiD_Lw"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sns.set_style('darkgrid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11W4IW3BO3Mx"
      },
      "source": [
        "root_path = '/content/'\n",
        "data_path ='ETTh1.csv'\n",
        "timeenc = 0\n",
        "frequency = 'h'\n",
        "features = 'MS'\n",
        "target = 'OT'\n",
        "seq_len = 96\n",
        "label_len = 48\n",
        "pred_len = 24\n",
        "scale = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "1OcgSqRSMV0k",
        "outputId": "96f503ce-6419-4b42-e230-f29e96300495"
      },
      "source": [
        "etth_data = pd.read_csv(root_path + data_path)\n",
        "etth_df = pd.DataFrame(etth_data)\n",
        "etth_df = etth_df.drop(['date'], axis=1)\n",
        "etth_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HUFL</th>\n",
              "      <th>HULL</th>\n",
              "      <th>MUFL</th>\n",
              "      <th>MULL</th>\n",
              "      <th>LUFL</th>\n",
              "      <th>LULL</th>\n",
              "      <th>OT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.827</td>\n",
              "      <td>2.009</td>\n",
              "      <td>1.599</td>\n",
              "      <td>0.462</td>\n",
              "      <td>4.203</td>\n",
              "      <td>1.340</td>\n",
              "      <td>30.531000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.693</td>\n",
              "      <td>2.076</td>\n",
              "      <td>1.492</td>\n",
              "      <td>0.426</td>\n",
              "      <td>4.142</td>\n",
              "      <td>1.371</td>\n",
              "      <td>27.787001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.157</td>\n",
              "      <td>1.741</td>\n",
              "      <td>1.279</td>\n",
              "      <td>0.355</td>\n",
              "      <td>3.777</td>\n",
              "      <td>1.218</td>\n",
              "      <td>27.787001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.090</td>\n",
              "      <td>1.942</td>\n",
              "      <td>1.279</td>\n",
              "      <td>0.391</td>\n",
              "      <td>3.807</td>\n",
              "      <td>1.279</td>\n",
              "      <td>25.044001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.358</td>\n",
              "      <td>1.942</td>\n",
              "      <td>1.492</td>\n",
              "      <td>0.462</td>\n",
              "      <td>3.868</td>\n",
              "      <td>1.279</td>\n",
              "      <td>21.948000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    HUFL   HULL   MUFL   MULL   LUFL   LULL         OT\n",
              "0  5.827  2.009  1.599  0.462  4.203  1.340  30.531000\n",
              "1  5.693  2.076  1.492  0.426  4.142  1.371  27.787001\n",
              "2  5.157  1.741  1.279  0.355  3.777  1.218  27.787001\n",
              "3  5.090  1.942  1.279  0.391  3.807  1.279  25.044001\n",
              "4  5.358  1.942  1.492  0.462  3.868  1.279  21.948000"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii8ubYm_QY6j"
      },
      "source": [
        "# sns.pairplot(data=etth_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "dy5vwc_JM-78",
        "outputId": "be01bc24-bd50-4809-f54f-ffa7e8a50b71"
      },
      "source": [
        "sns.lineplot(data=etth_data['OT'][16500:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f82bcc9e050>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaiFEkkjKfpO"
      },
      "source": [
        "def time_features(dates, timeenc, frequency):\n",
        "   if timeenc == 0:\n",
        "     dates['month'] = dates.date.apply(lambda row: row.month, 1)\n",
        "     dates['day'] = dates.date.apply(lambda row: row.day, 1)\n",
        "     dates['weekday'] = dates.date.apply(lambda row: row.weekday(), 1)\n",
        "     dates['hour'] = dates.date.apply(lambda row: row.hour, 1)\n",
        "     dates['minute'] = dates.date.apply(lambda row: row.minute, 1)\n",
        "     dates['minute'] = dates.minute.map(lambda x: x // 15)\n",
        "     frequency_map = {'y':[], 'm':['month'], 'w':['month'], 'd':['month','day','weekday'], \n",
        "                      'b':['month','day','weekday'], 'h':['month','day','weekday','hour'],\n",
        "                      't':['month','day','weekday','hour','minute']\n",
        "                      }\n",
        "     return dates[frequency_map[frequency.lower()]].values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4kHBcL9XWTa"
      },
      "source": [
        "def read_data(df_raw, features, flag_id) -> Tuple[np.ndarray, np.ndarray]:\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  \n",
        "  #Train/Test/Val = 12/4/4 months\n",
        "  border1s = [0, 12*30*24 - seq_len,  12*30*24+4*30*24 - seq_len]\n",
        "  border2s = [12*30*24, 12*30*24+4*30*24, 12*30*24+8*30*24]\n",
        "  \n",
        "  border1 = border1s[flag_id]\n",
        "  border2 = border2s[flag_id]\n",
        "\n",
        "  if features == \"M\" or features == \"MS\":\n",
        "    cols = df_raw.columns[1:]\n",
        "    df_data = df_raw[cols]\n",
        "  elif features == \"S\":\n",
        "    df_data = df_raw[[target]]\n",
        " \n",
        "  if scale:\n",
        "    train_data = df_data[border1s[0] : border2s[0]]\n",
        "    scaler.fit(train_data)\n",
        "    data = scaler.transform(df_data.values)\n",
        "  else:\n",
        "    data = df_data.values\n",
        " \n",
        "  df_stamp = df_raw[['date']][border1:border2]\n",
        "  df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
        "\n",
        "  data_stamp = time_features(df_stamp, timeenc, frequency)\n",
        "  data_x = data[border1:border2]\n",
        "\n",
        "  return data_x, data_stamp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvpax089T9xj"
      },
      "source": [
        "def split_window(window) -> tf.Tensor:\n",
        "  y_start = seq_len - label_len\n",
        "  y_end = y_start + label_len + pred_len\n",
        "  return window[:seq_len], window[y_start:y_end]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCyvdOJbUBCU"
      },
      "source": [
        "def process_window(window, window_mark) -> Tuple[List[tf.Tensor], tf.Tensor]:\n",
        "   x_enc = window[0]\n",
        "   x_dec = tf.identity(window[1])\n",
        "\n",
        "   x_dec = tf.concat([x_dec[:-pred_len], tf.zeros((pred_len, x_dec.shape[1]), dtype=tf.float64)], 0)\n",
        "\n",
        "   x_mark_enc = window_mark[0]\n",
        "   x_mark_dec = window_mark[1]\n",
        "   \n",
        "   y = window[1][-pred_len:]\n",
        "   \n",
        "   return (x_enc, x_dec, x_mark_enc, x_mark_dec), y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0eNg0fUUCId"
      },
      "source": [
        "def create_dataset(data, drop_remainder) -> tf.data.Dataset:\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "\n",
        "  dataset = dataset.window(seq_len + pred_len, shift=1, drop_remainder=drop_remainder)\n",
        "  dataset = dataset.flat_map(lambda window: window.batch(seq_len + pred_len))\n",
        "  \n",
        "  dataset = dataset.map(split_window, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHKDBO1UEJxe"
      },
      "source": [
        "def CustomDataset(flag='train', batch_size = 32, drop_remainder=True) -> tf.data.Dataset:\n",
        "  assert flag in ['train', 'test', 'val']\n",
        "  type_map = {'train': 0, 'val': 1, 'test': 2}\n",
        "  \n",
        "  df_raw = pd.read_csv(os.path.join(root_path, data_path))\n",
        "  \n",
        "  data_x, data_stamp = read_data(df_raw, features, type_map[flag])\n",
        "\n",
        "  ds_features = create_dataset(data_x, drop_remainder)\n",
        "  ds_stamp = create_dataset(data_stamp, drop_remainder)\n",
        "  \n",
        "  dataset = tf.data.Dataset.zip((ds_features, ds_stamp))\n",
        "  dataset = dataset.map(process_window, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
        "  \n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVNb-y5Lc5Q2"
      },
      "source": [
        "# **Training, Validation & Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNjzadTBeEo9"
      },
      "source": [
        "import datetime\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH8X36AnVJVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c93e588-adfc-4e69-d958-59c663214db0"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 8\n",
        "\n",
        "ds_train = CustomDataset('train', BATCH_SIZE)\n",
        "ds_val = CustomDataset('val', BATCH_SIZE)\n",
        "ds_test = CustomDataset('test', BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:4212: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:4212: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:4212: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeKJOihk5c2k"
      },
      "source": [
        "# for (e, d, mE, mD), y in ds_train.take(1):\n",
        "#   print(f'Encoder input: {e[0].numpy()}\\n\\nDecoder Input: {d[0].numpy()}\\n\\nLabel: {y[0].numpy()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDDTj2fJc47z"
      },
      "source": [
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "checkpoint_dir = \"weights_checkpoint\"\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=7)\n",
        "lr_scheduler = LearningRateScheduler(lambda epoch: 1e-4 * (0.5 ** ((epoch -1) // 1)))\n",
        "model_checkpoint = ModelCheckpoint(checkpoint_dir, monitor='loss', save_best_only=True, save_weights_only=True)\n",
        "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "etth_model = Informer(enc_layers=3, dec_layers=5, training=True)\n",
        "\n",
        "etth_model.compile(optimizer= Adam(learning_rate=1e-4), loss='mse', metrics=['mse', 'mae'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqGf_jkikZ5W"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHriWl4Tgylf"
      },
      "source": [
        "# tf.keras.backend.clear_session()\n",
        "# etth_history = etth_model.fit(ds_train, validation_data=ds_val, epochs=50, callbacks=[early_stopping, model_checkpoint, tensorboard])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ4nDAumB7mQ"
      },
      "source": [
        "# tf.saved_model.save(etth_model, 'etth_saved_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzZiP92cLa65"
      },
      "source": [
        "# **Hyperparameter Tuning**\n",
        "Initial loss value matters, the lesser the initial loss value from the first epoch, the faster we can reach a better MSE before starting to overfit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gp0kltTLbr3",
        "outputId": "e69f6cc2-034b-4865-fdcf-bdf9a3c968a9"
      },
      "source": [
        "pip install -q -U keras-tuner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |                            | 10 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |                         | 20 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |                      | 30 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |                  | 40 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |               | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |            | 61 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |        | 71 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |     | 81 kB 4.8 MB/s eta 0:00:01\r\u001b[K     | | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     || 97 kB 3.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-PmbYKlLdr5"
      },
      "source": [
        "import keras_tuner as kt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g18a0KLhLfW5"
      },
      "source": [
        "def model_builder(hp):\n",
        "  hp_enc_layers = hp.Int('num_encoder_layers', 1, 3, default=1)\n",
        "  hp_dec_layers = hp.Int('num_decoder_layers', 2, 5, default=2)\n",
        "  hp_dropout_rate = hp.Float('dropout_rate', 0, 0.5, step=0.01, default=0.05)\n",
        "  hp_expansion = hp.Int('expansion', 1, 4, default=4)\n",
        "  hp_factor = hp.Int('factor', 1, 10, default=5)\n",
        "  hp_learning_rate = hp.Fixed('learning_rate', 1e-4)\n",
        "\n",
        "  kt_etth_model = Informer(enc_layers=hp_enc_layers, dec_layers=hp_dec_layers, dropout_rate=hp_dropout_rate, \n",
        "                           factor = hp_factor, expansion = hp_expansion, training=True)\n",
        "  kt_etth_model.compile(optimizer= Adam(learning_rate=hp_learning_rate), loss='mse', metrics=['mse'])\n",
        "\n",
        "  return kt_etth_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vwsurNaLhPi",
        "outputId": "c669f7ad-579c-430c-dbfc-ef89f103d02d"
      },
      "source": [
        "kt_early_stopping = EarlyStopping(monitor='loss', patience=4)\n",
        "kt_lr_scheduler = LearningRateScheduler(lambda epoch: 1e-4 * (0.5 ** ((epoch -1) // 1)))\n",
        "\n",
        "tuner = kt.Hyperband(model_builder, objective='val_mse', max_epochs=10, factor=3, directory='/content/', project_name='kt_etth_tuning')\n",
        "tuner.search(ds_train, validation_data=ds_val, epochs=50, callbacks=[kt_early_stopping, kt_lr_scheduler])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 23m 11s]\n",
            "val_mse: 1.6019867658615112\n",
            "\n",
            "Best val_mse So Far: 1.6019867658615112\n",
            "Total elapsed time: 05h 38m 49s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    }
  ]
}