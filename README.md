# Informer
Implementation of the Informer paper for LSTF  using transformer architecture
The current Tensorflow implementation of the ProbSparseAttention produces inconsistent
results when compared with the author's [implementation](https://github.com/zhouhaoyi/Informer2020) (As noted on August 2020).
Therefore I created a Tensorflow equivalent while preseving consistency of the attentionmechanism. 
[Read the author's paper here.](https://arxiv.org/abs/2012.07436)
